{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-08-15T10:03:35.494057Z","iopub.execute_input":"2024-08-15T10:03:35.494700Z","iopub.status.idle":"2024-08-15T10:03:48.684246Z","shell.execute_reply.started":"2024-08-15T10:03:35.494665Z","shell.execute_reply":"2024-08-15T10:03:48.683024Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom datasets import load_dataset, Dataset\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer, default_data_collator, RobertaTokenizer\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport nltk\n\n# Ensure necessary downloads for BLEU metric\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2024-08-15T10:03:48.686737Z","iopub.execute_input":"2024-08-15T10:03:48.687164Z","iopub.status.idle":"2024-08-15T10:03:56.405972Z","shell.execute_reply.started":"2024-08-15T10:03:48.687126Z","shell.execute_reply":"2024-08-15T10:03:56.405077Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-15 10:03:52.486417: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 10:03:52.486475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 10:03:52.487844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Load the CodeSearchNet dataset for each language\ndataset_java = load_dataset('code_search_net', 'java')['train']\ndataset_python = load_dataset('code_search_net', 'python')['train']\ndataset_go = load_dataset('code_search_net', 'go')['train']","metadata":{"execution":{"iopub.status.busy":"2024-08-15T10:03:56.407192Z","iopub.execute_input":"2024-08-15T10:03:56.407768Z","iopub.status.idle":"2024-08-15T10:20:24.122433Z","shell.execute_reply.started":"2024-08-15T10:03:56.407741Z","shell.execute_reply":"2024-08-15T10:20:24.121505Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9565b0759e8c4f18a426d0b0db82a95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/12.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39ffb638a0e47a0a7bfff7db23fea28"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90671e97de94d998042a30a7f71d7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/454451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1918c54529df47768dbbba900a06d998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/26909 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22542b98639949b3ad9b5d0c5d19219f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/15328 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3723edaef4e849d9923a250e83ad7139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e1e96da556428abdfb58e41c1cde5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/412178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ec64314dea4f4ab7311ab525f7a8e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/22176 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b484c037037e4129864d6e4e54ba9523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/23107 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38c90863b144c689be8280c39380321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/488M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e569da023e524cf09436e1619b8d2cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/317832 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d0503ce69c4178a31329b74c876e9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/14291 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8449a49d4cd4d4cb41f07d246f59212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/14242 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"095df14bf880459aad1ada38014f379c"}},"metadata":{}}]},{"cell_type":"code","source":"# Function to sample 100 examples\ndef sample_data(dataset, num_samples=10000):\n    sampled_indices = random.sample(range(len(dataset)), num_samples)\n    return dataset.select(sampled_indices)\n\n# Sample 100 pairs from each language\nsampled_java = sample_data(dataset_java)\nsampled_python = sample_data(dataset_python)\nsampled_go = sample_data(dataset_go)\n\n# Function to split data into train, validation, and test sets\ndef train_val_test_split_data(dataset, val_size=0.1, test_size=0.1):\n    train_size = 1 - (val_size + test_size)\n    train_dataset = dataset.shuffle(seed=42).select(range(int(train_size * len(dataset))))\n    temp_dataset = dataset.shuffle(seed=42).select(range(int(train_size * len(dataset)), len(dataset)))\n    split = int(len(temp_dataset) * (test_size / (test_size + val_size)))\n    val_dataset = temp_dataset.select(range(len(temp_dataset) - split))\n    test_dataset = temp_dataset.select(range(len(temp_dataset) - split, len(temp_dataset)))\n    return train_dataset, val_dataset, test_dataset\n\n# Split the data into train, validation, and test sets for each language\ntrain_java, val_java, test_java = train_val_test_split_data(sampled_java)\ntrain_python, val_python, test_python = train_val_test_split_data(sampled_python)\ntrain_go, val_go, test_go = train_val_test_split_data(sampled_go)\n\n# Load the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n\n# Preprocess function\ndef preprocess_function(examples):\n    inputs = examples['func_code_string']\n    targets = examples['func_documentation_string']\n    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\n# Preprocess the datasets\ntrain_java = train_java.map(preprocess_function, batched=True)\nval_java = val_java.map(preprocess_function, batched=True)\ntest_java = test_java.map(preprocess_function, batched=True)\n\ntrain_python = train_python.map(preprocess_function, batched=True)\nval_python = val_python.map(preprocess_function, batched=True)\ntest_python = test_python.map(preprocess_function, batched=True)\n\ntrain_go = train_go.map(preprocess_function, batched=True)\nval_go = val_go.map(preprocess_function, batched=True)\ntest_go = test_go.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:25:29.974810Z","iopub.execute_input":"2024-08-15T11:25:29.975473Z","iopub.status.idle":"2024-08-15T11:27:24.285438Z","shell.execute_reply.started":"2024-08-15T11:25:29.975439Z","shell.execute_reply":"2024-08-15T11:27:24.284480Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5496157b0634e4b84bac44b7f1b592a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc6014c4bf2646eca473fe16619c881f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f7b42adadf47abb04082ce1a12a25b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6835c76e4884b6c86f1c37d68bfd2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e78e7852234ce4a1423e15201cbe7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff59a82098a4938b7b108f0f6124e6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e37e4e256f40f0942c523975f7f100"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c9a6fa1618942c799537b4cda44e65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b6617516bf4fa287b01e652ec863a2"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import T5ForConditionalGeneration, TrainingArguments, default_data_collator\nimport nltk\n\n# Ensure necessary downloads for BLEU metric\nnltk.download('punkt')\n\n# Custom collate function to handle different data types\ndef custom_collate_fn(features):\n    batch = {}\n    for k in features[0].keys():\n        if isinstance(features[0][k], list) and isinstance(features[0][k][0], int):  # Tokenized data (assumed to be lists of integers)\n            batch[k] = torch.tensor([f[k] for f in features])\n        elif isinstance(features[0][k], torch.Tensor):  # Already tensors\n            batch[k] = torch.stack([f[k] for f in features])\n        else:\n            batch[k] = [f[k] for f in features]  # Keep as a list (likely for non-tensor data like strings)\n    \n    return batch\n\n# EWC Loss function\nclass EWCLoss(nn.Module):\n    def __init__(self, model, old_params, fisher_matrix, lambda_=0.4):\n        super(EWCLoss, self).__init__()\n        self.model = model\n        self.old_params = old_params\n        self.fisher_matrix = fisher_matrix\n        self.lambda_ = lambda_\n    \n    def forward(self, current_loss):\n        ewc_loss = 0\n        for name, param in self.model.named_parameters():\n            if name in self.fisher_matrix:\n                ewc_loss += torch.sum(self.fisher_matrix[name] * (param - self.old_params[name]).pow(2))\n        total_loss = current_loss + self.lambda_ * ewc_loss\n        return total_loss\n\ndef compute_fisher_information(model, dataloader, device):\n    fisher_matrix = {}\n    for name, param in model.named_parameters():\n        fisher_matrix[name] = torch.zeros_like(param)\n\n    model.eval()\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        model.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        for name, param in model.named_parameters():\n            fisher_matrix[name] += param.grad.pow(2)\n\n    for name in fisher_matrix:\n        fisher_matrix[name] = fisher_matrix[name] / len(dataloader)\n\n    return fisher_matrix\n\ndef save_old_params(model):\n    old_params = {}\n    for name, param in model.named_parameters():\n        old_params[name] = param.clone().detach()\n    return old_params\n\n# Function to fine-tune the model with EWC\ndef fine_tune_with_ewc(model, train_dataset, val_dataset, output_dir, old_params=None, fisher_matrix=None):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,  # Reduced batch size to fit in memory\n        per_device_eval_batch_size=4,   # Reduced batch size to fit in memory\n        evaluation_strategy=\"epoch\",\n        save_steps=1000,\n        save_total_limit=2,\n        logging_dir='./logs',\n        gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n        fp16=True,  # Enable mixed precision training to save memory\n    )\n    \n    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\n    if old_params and fisher_matrix:\n        ewc_loss = EWCLoss(model, old_params, fisher_matrix, lambda_=0.4)\n\n    # Use the custom collate function\n    train_dataloader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, collate_fn=custom_collate_fn)\n\n    for epoch in range(training_args.num_train_epochs):\n        model.train()\n        model.to(device)  # Ensure model is on the correct device\n        torch.cuda.empty_cache()  # Clear cache to free up memory\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}  # Ensure batch is on the correct device\n            outputs = model(**batch)\n            loss = outputs.loss\n\n            if old_params and fisher_matrix:\n                loss = ewc_loss(loss)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    return model\n\n# Function to evaluate the model using BLEU score\ndef evaluate_model(model, test_dataset, device):\n    model.to(device)  # Ensure model is on the correct device\n    torch.cuda.empty_cache()  # Clear cache to free up memory\n    test_results = []\n    \n    for example in test_dataset:\n        inputs = tokenizer(example['func_code_string'], return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        outputs = model.generate(**inputs)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        reference = example['func_documentation_string']\n        test_results.append({\"prediction\": prediction, \"reference\": reference})\n    \n    predictions = [result[\"prediction\"] for result in test_results]\n    references = [nltk.word_tokenize(result[\"reference\"]) for result in test_results]\n    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n    \n    bleu = nltk.translate.bleu_score.corpus_bleu([[ref] for ref in references], tokenized_predictions)\n    \n    return {\"bleu\": bleu}\n\n# Define the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained CodeT5 model\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n\n# Step 1: Fine-tune on Java\nmodel = fine_tune_with_ewc(model, train_java, val_java, './results_java')\nfisher_matrix_java = compute_fisher_information(model, DataLoader(val_java, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_java = save_old_params(model)\n\n# Step 2: Fine-tune on Python with EWC\nmodel = fine_tune_with_ewc(model, train_python, val_python, './results_python', old_params_java, fisher_matrix_java)\nfisher_matrix_python = compute_fisher_information(model, DataLoader(val_python, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_python = save_old_params(model)\n\n# Step 3: Fine-tune on Go with EWC\nmodel = fine_tune_with_ewc(model, train_go, val_go, './results_go', old_params_python, fisher_matrix_python)\n\n# Evaluate the model\njava_results_after_go = evaluate_model(model, test_java, device)\npython_results_after_go = evaluate_model(model, test_python, device)\ngo_results_after_go = evaluate_model(model, test_go, device)\n\n# Display results\nprint(\"Results after training on Java, Python, and Go with EWC:\")\nprint(\"Java Test Results:\", java_results_after_go)\nprint(\"Python Test Results:\", python_results_after_go)\nprint(\"Go Test Results:\", go_results_after_go)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T10:30:31.860797Z","iopub.execute_input":"2024-08-15T10:30:31.861142Z","iopub.status.idle":"2024-08-15T10:44:13.200594Z","shell.execute_reply.started":"2024-08-15T10:30:31.861115Z","shell.execute_reply":"2024-08-15T10:44:13.199635Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Results after training on Java, Python, and Go with EWC:\nJava Test Results: {'bleu': 0.0032440345624518857}\nPython Test Results: {'bleu': 0.0111070966258134}\nGo Test Results: {'bleu': 0.09627148154406495}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import T5ForConditionalGeneration, TrainingArguments, default_data_collator\nimport nltk\n\n# Ensure necessary downloads for BLEU metric\nnltk.download('punkt')\n\n# Custom collate function to handle different data types\ndef custom_collate_fn(features):\n    batch = {}\n    for k in features[0].keys():\n        if isinstance(features[0][k], list) and isinstance(features[0][k][0], int):  # Tokenized data (assumed to be lists of integers)\n            batch[k] = torch.tensor([f[k] for f in features])\n        elif isinstance(features[0][k], torch.Tensor):  # Already tensors\n            batch[k] = torch.stack([f[k] for f in features])\n        else:\n            batch[k] = [f[k] for f in features]  # Keep as a list (likely for non-tensor data like strings)\n    \n    return batch\n\n# EWC Loss function\nclass EWCLoss(nn.Module):\n    def __init__(self, model, old_params, fisher_matrix, lambda_=0.4):\n        super(EWCLoss, self).__init__()\n        self.model = model\n        self.old_params = old_params\n        self.fisher_matrix = fisher_matrix\n        self.lambda_ = lambda_\n    \n    def forward(self, current_loss):\n        ewc_loss = 0\n        for name, param in self.model.named_parameters():\n            if name in self.fisher_matrix:\n                ewc_loss += torch.sum(self.fisher_matrix[name] * (param - self.old_params[name]).pow(2))\n        total_loss = current_loss + self.lambda_ * ewc_loss\n        return total_loss\n\ndef compute_fisher_information(model, dataloader, device):\n    fisher_matrix = {}\n    for name, param in model.named_parameters():\n        fisher_matrix[name] = torch.zeros_like(param)\n\n    model.eval()\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        model.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        for name, param in model.named_parameters():\n            fisher_matrix[name] += param.grad.pow(2)\n\n    for name in fisher_matrix:\n        fisher_matrix[name] = fisher_matrix[name] / len(dataloader)\n\n    return fisher_matrix\n\ndef save_old_params(model):\n    old_params = {}\n    for name, param in model.named_parameters():\n        old_params[name] = param.clone().detach()\n    return old_params\n\n# Function to fine-tune the model with EWC\ndef fine_tune_with_ewc(model, train_dataset, val_dataset, output_dir, old_params=None, fisher_matrix=None):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,  # Reduced batch size to fit in memory\n        per_device_eval_batch_size=4,   # Reduced batch size to fit in memory\n        evaluation_strategy=\"epoch\",\n        save_steps=1000,\n        save_total_limit=2,\n        logging_dir='./logs',\n        gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n        fp16=True,  # Enable mixed precision training to save memory\n    )\n    \n    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\n    if old_params and fisher_matrix:\n        ewc_loss = EWCLoss(model, old_params, fisher_matrix, lambda_=0.4)\n\n    # Use the custom collate function\n    train_dataloader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, collate_fn=custom_collate_fn)\n\n    for epoch in range(training_args.num_train_epochs):\n        model.train()\n        model.to(device)  # Ensure model is on the correct device\n        torch.cuda.empty_cache()  # Clear cache to free up memory\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}  # Ensure batch is on the correct device\n            outputs = model(**batch)\n            loss = outputs.loss\n\n            if old_params and fisher_matrix:\n                loss = ewc_loss(loss)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    return model\n\n# Function to evaluate the model using BLEU score\ndef evaluate_model(model, test_dataset, device):\n    model.to(device)  # Ensure model is on the correct device\n    torch.cuda.empty_cache()  # Clear cache to free up memory\n    test_results = []\n    \n    for example in test_dataset:\n        inputs = tokenizer(example['func_code_string'], return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        outputs = model.generate(**inputs)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        reference = example['func_documentation_string']\n        test_results.append({\"prediction\": prediction, \"reference\": reference})\n    \n    predictions = [result[\"prediction\"] for result in test_results]\n    references = [nltk.word_tokenize(result[\"reference\"]) for result in test_results]\n    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n    \n    bleu = nltk.translate.bleu_score.corpus_bleu([[ref] for ref in references], tokenized_predictions)\n    \n    return {\"bleu\": bleu}\n\n# Define the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained CodeT5 model\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n\n# Step 1: Fine-tune on Java\nmodel = fine_tune_with_ewc(model, train_java, val_java, './results_java')\nfisher_matrix_java = compute_fisher_information(model, DataLoader(val_java, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_java = save_old_params(model)\n\n# Evaluate on all languages after training on Java\njava_results_after_java = evaluate_model(model, test_java, device)\npython_results_after_java = evaluate_model(model, test_python, device)\ngo_results_after_java = evaluate_model(model, test_go, device)\n\n# Step 2: Fine-tune on Python with EWC\nmodel = fine_tune_with_ewc(model, train_python, val_python, './results_python', old_params_java, fisher_matrix_java)\nfisher_matrix_python = compute_fisher_information(model, DataLoader(val_python, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_python = save_old_params(model)\n\n# Evaluate on all languages after training on Python\njava_results_after_python = evaluate_model(model, test_java, device)\npython_results_after_python = evaluate_model(model, test_python, device)\ngo_results_after_python = evaluate_model(model, test_go, device)\n\n# Step 3: Fine-tune on Go with EWC\nmodel = fine_tune_with_ewc(model, train_go, val_go, './results_go', old_params_python, fisher_matrix_python)\n\n# Evaluate on all languages after training on Go\njava_results_after_go = evaluate_model(model, test_java, device)\npython_results_after_go = evaluate_model(model, test_python, device)\ngo_results_after_go = evaluate_model(model, test_go, device)\n\n# Display results after training on Java\nprint(\"Results after training on Java:\")\nprint(\"Java Test Results:\", java_results_after_java)\nprint(\"Python Test Results:\", python_results_after_java)\nprint(\"Go Test Results:\", go_results_after_java)\n\n# Display results after training on Python\nprint(\"\\nResults after training on Python:\")\nprint(\"Java Test Results:\", java_results_after_python)\nprint(\"Python Test Results:\", python_results_after_python)\nprint(\"Go Test Results:\", go_results_after_python)\n\n# Display results after training on Go\nprint(\"\\nResults after training on Go:\")\nprint(\"Java Test Results:\", java_results_after_go)\nprint(\"Python Test Results:\", python_results_after_go)\nprint(\"Go Test Results:\", go_results_after_go)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T10:44:13.202251Z","iopub.execute_input":"2024-08-15T10:44:13.202563Z","iopub.status.idle":"2024-08-15T11:00:23.624278Z","shell.execute_reply.started":"2024-08-15T10:44:13.202529Z","shell.execute_reply":"2024-08-15T11:00:23.623167Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Results after training on Java:\nJava Test Results: {'bleu': 0.026341522383927164}\nPython Test Results: {'bleu': 0.009722728672314655}\nGo Test Results: {'bleu': 0.02905000989726947}\n\nResults after training on Python:\nJava Test Results: {'bleu': 0.001360040253230163}\nPython Test Results: {'bleu': 0.022398872216613276}\nGo Test Results: {'bleu': 0.0011622314351092589}\n\nResults after training on Go:\nJava Test Results: {'bleu': 0.0023664290539881752}\nPython Test Results: {'bleu': 0.00622360050249214}\nGo Test Results: {'bleu': 0.10808743104359486}\n","output_type":"stream"}]},{"cell_type":"code","source":"#1000 codes, ewc, normal bleu","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom transformers import T5ForConditionalGeneration, TrainingArguments, default_data_collator\nimport nltk\nfrom nltk.translate.bleu_score import SmoothingFunction\n\n# Ensure necessary downloads for BLEU metric\nnltk.download('punkt')\n\n# Custom collate function to handle different data types\ndef custom_collate_fn(features):\n    batch = {}\n    for k in features[0].keys():\n        if isinstance(features[0][k], list) and isinstance(features[0][k][0], int):  # Tokenized data (assumed to be lists of integers)\n            batch[k] = torch.tensor([f[k] for f in features])\n        elif isinstance(features[0][k], torch.Tensor):  # Already tensors\n            batch[k] = torch.stack([f[k] for f in features])\n        else:\n            batch[k] = [f[k] for f in features]  # Keep as a list (likely for non-tensor data like strings)\n    \n    return batch\n\n# EWC Loss function\nclass EWCLoss(nn.Module):\n    def __init__(self, model, old_params, fisher_matrix, lambda_=0.4):\n        super(EWCLoss, self).__init__()\n        self.model = model\n        self.old_params = old_params\n        self.fisher_matrix = fisher_matrix\n        self.lambda_ = lambda_\n    \n    def forward(self, current_loss):\n        ewc_loss = 0\n        for name, param in self.model.named_parameters():\n            if name in self.fisher_matrix:\n                ewc_loss += torch.sum(self.fisher_matrix[name] * (param - self.old_params[name]).pow(2))\n        total_loss = current_loss + self.lambda_ * ewc_loss\n        return total_loss\n\ndef compute_fisher_information(model, dataloader, device):\n    fisher_matrix = {}\n    for name, param in model.named_parameters():\n        fisher_matrix[name] = torch.zeros_like(param)\n\n    model.eval()\n    for batch in dataloader:\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        model.zero_grad()\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        for name, param in model.named_parameters():\n            fisher_matrix[name] += param.grad.pow(2)\n\n    for name in fisher_matrix:\n        fisher_matrix[name] = fisher_matrix[name] / len(dataloader)\n\n    return fisher_matrix\n\ndef save_old_params(model):\n    old_params = {}\n    for name, param in model.named_parameters():\n        old_params[name] = param.clone().detach()\n    return old_params\n\n# Function to fine-tune the model with EWC\ndef fine_tune_with_ewc(model, train_dataset, val_dataset, output_dir, old_params=None, fisher_matrix=None):\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,  # Reduced batch size to fit in memory\n        per_device_eval_batch_size=4,   # Reduced batch size to fit in memory\n        evaluation_strategy=\"epoch\",\n        save_steps=1000,\n        save_total_limit=2,\n        logging_dir='./logs',\n        gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n        fp16=True,  # Enable mixed precision training to save memory\n    )\n    \n    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\n    if old_params and fisher_matrix:\n        ewc_loss = EWCLoss(model, old_params, fisher_matrix, lambda_=0.4)\n\n    # Use the custom collate function\n    train_dataloader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, collate_fn=custom_collate_fn)\n\n    for epoch in range(training_args.num_train_epochs):\n        model.train()\n        model.to(device)  # Ensure model is on the correct device\n        torch.cuda.empty_cache()  # Clear cache to free up memory\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}  # Ensure batch is on the correct device\n            outputs = model(**batch)\n            loss = outputs.loss\n\n            if old_params and fisher_matrix:\n                loss = ewc_loss(loss)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n    return model\n\n# Function to evaluate the model using Smoothed BLEU score\ndef evaluate_model(model, test_dataset, device):\n    model.to(device)  # Ensure model is on the correct device\n    torch.cuda.empty_cache()  # Clear cache to free up memory\n    test_results = []\n    \n    for example in test_dataset:\n        inputs = tokenizer(example['func_code_string'], return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        outputs = model.generate(**inputs)\n        prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        reference = example['func_documentation_string']\n        test_results.append({\"prediction\": prediction, \"reference\": reference})\n    \n    predictions = [result[\"prediction\"] for result in test_results]\n    references = [nltk.word_tokenize(result[\"reference\"]) for result in test_results]\n    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n    \n    # Compute smoothed BLEU scores\n    smoothing_function = SmoothingFunction().method1\n    bleu = nltk.translate.bleu_score.corpus_bleu([[ref] for ref in references], tokenized_predictions, smoothing_function=smoothing_function)\n    \n    return {\"bleu\": bleu}\n\n# Define the device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained CodeT5 model\nmodel = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n\n# Step 1: Fine-tune on Java\nmodel = fine_tune_with_ewc(model, train_java, val_java, './results_java')\nfisher_matrix_java = compute_fisher_information(model, DataLoader(val_java, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_java = save_old_params(model)\n\n# Evaluate on all languages after training on Java\njava_results_after_java = evaluate_model(model, test_java, device)\npython_results_after_java = evaluate_model(model, test_python, device)\ngo_results_after_java = evaluate_model(model, test_go, device)\n\n# Step 2: Fine-tune on Python with EWC\nmodel = fine_tune_with_ewc(model, train_python, val_python, './results_python', old_params_java, fisher_matrix_java)\nfisher_matrix_python = compute_fisher_information(model, DataLoader(val_python, batch_size=4, collate_fn=custom_collate_fn), device)\nold_params_python = save_old_params(model)\n\n# Evaluate on all languages after training on Python\njava_results_after_python = evaluate_model(model, test_java, device)\npython_results_after_python = evaluate_model(model, test_python, device)\ngo_results_after_python = evaluate_model(model, test_go, device)\n\n# Step 3: Fine-tune on Go with EWC\nmodel = fine_tune_with_ewc(model, train_go, val_go, './results_go', old_params_python, fisher_matrix_python)\n\n# Evaluate on all languages after training on Go\njava_results_after_go = evaluate_model(model, test_java, device)\npython_results_after_go = evaluate_model(model, test_python, device)\ngo_results_after_go = evaluate_model(model, test_go, device)\n\n# Display results after training on Java\nprint(\"Results after training on Java:\")\nprint(\"Java Test Results:\", java_results_after_java)\nprint(\"Python Test Results:\", python_results_after_java)\nprint(\"Go Test Results:\", go_results_after_java)\n\n# Display results after training on Python\nprint(\"\\nResults after training on Python:\")\nprint(\"Java Test Results:\", java_results_after_python)\nprint(\"Python Test Results:\", python_results_after_python)\nprint(\"Go Test Results:\", go_results_after_python)\n\n# Display results after training on Go\nprint(\"\\nResults after training on Go:\")\nprint(\"Java Test Results:\", java_results_after_go)\nprint(\"Python Test Results:\", python_results_after_go)\nprint(\"Go Test Results:\", go_results_after_go)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-15T11:27:24.287612Z","iopub.execute_input":"2024-08-15T11:27:24.287909Z","iopub.status.idle":"2024-08-15T14:11:19.455382Z","shell.execute_reply.started":"2024-08-15T11:27:24.287884Z","shell.execute_reply":"2024-08-15T14:11:19.454402Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Results after training on Java:\nJava Test Results: {'bleu': 0.019047554191201493}\nPython Test Results: {'bleu': 0.014187685572252473}\nGo Test Results: {'bleu': 0.01276626324217596}\n\nResults after training on Python:\nJava Test Results: {'bleu': 0.0003615596519135391}\nPython Test Results: {'bleu': 0.03070638629396841}\nGo Test Results: {'bleu': 0.0001992415818252553}\n\nResults after training on Go:\nJava Test Results: {'bleu': 0.004034870505773016}\nPython Test Results: {'bleu': 0.011322548164213471}\nGo Test Results: {'bleu': 0.058062633590045765}\n","output_type":"stream"}]},{"cell_type":"code","source":"#10000 codes, ewc, smoothed bleu","metadata":{},"execution_count":null,"outputs":[]}]}